{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import itertools\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from experiments.collect_evaluation_data import define_AM_strategies, get_combo_cfg\n",
    "from experiments.eval_experiments import EVAL_EXPERIMENTS\n",
    "from experiments.eval_utils import (\n",
    "    generate_combinations,\n",
    "    path_from_cfg,\n",
    "    jaccard,\n",
    "    get_auroc,\n",
    "    ssim_dist,\n",
    "    alex_lpips,\n",
    "    mse_dist,\n",
    ")\n",
    "from models import evaluate, get_encodings\n",
    "from core.custom_dataset import CustomDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from core.manipulation_set import FrequencyManipulationSet, RGBManipulationSet\n",
    "from plotting import (\n",
    "    collect_fv_data,\n",
    "    fv_2d_grid_step_vs_model,\n",
    "    collect_fv_data_by_step,\n",
    "    fv_2d_grid_model_depth_vs_width,\n",
    "    fv_grid_different_targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.ioff()\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/Library/TeX/texbin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(27)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.set_palette(\"pastel\")\n",
    "sns.set(font_scale=1.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOP_K = 4\n",
    "STRATEGY = \"Adam + GC + TR\"\n",
    "SAVE_PATH = \"./results/dataframes/\"\n",
    "param_grid = EVAL_EXPERIMENTS[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg_name = param_grid.pop(\"cfg_name\", \"config\")\n",
    "cfg_path = param_grid.pop(\"cfg_path\", \"./config\")\n",
    "name = param_grid.pop(\"name\", \"\")\n",
    "\n",
    "combinations = list(generate_combinations(param_grid))\n",
    "\n",
    "cfg, overrides = get_combo_cfg(cfg_name, cfg_path, {})\n",
    "device = \"cuda:0\"\n",
    "\n",
    "strategy = cfg.get(\"strategy\", STRATEGY)\n",
    "original_weights = cfg.model.get(\"original_weights_path\", None)\n",
    "if original_weights:\n",
    "    original_weights = \"{}/{}\".format(cfg.model_dir, original_weights)\n",
    "man_alpha = cfg.alpha\n",
    "data_dir = cfg.data_dir\n",
    "dataset = cfg.data\n",
    "image_dims = cfg.data.image_dims\n",
    "n_channels = cfg.data.n_channels\n",
    "class_dict_file = cfg.data.get(\"class_dict_file\", None)\n",
    "if class_dict_file is not None:\n",
    "    class_dict_file = class_dict_file\n",
    "fv_domain = cfg.fv_domain\n",
    "batch_size = cfg.batch_size\n",
    "cfg.target_img_path = \".\" + cfg.target_img_path\n",
    "\n",
    "if \"target_act_fn\" in cfg.model:\n",
    "    target_act_fn = hydra.utils.instantiate(cfg.model.target_act_fn)\n",
    "else:\n",
    "    target_act_fn = lambda x: x\n",
    "\n",
    "img_path = Path(cfg.target_img_path)\n",
    "\n",
    "layer_str = cfg.model.layer\n",
    "target_neuron = int(cfg.model.target_neuron)\n",
    "\n",
    "image_transforms = hydra.utils.instantiate(dataset.fv_transforms)\n",
    "normalize = hydra.utils.instantiate(cfg.data.normalize)\n",
    "denormalize = hydra.utils.instantiate(cfg.data.denormalize)\n",
    "resize_transforms = hydra.utils.instantiate(cfg.data.resize_transforms)\n",
    "\n",
    "save_path = f\"{SAVE_PATH}/{cfg_name}/{name}_{strategy}/\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_path = f\"../results/figures/{cfg_name}/{name}_{STRATEGY}/\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noise_ds_type = FrequencyManipulationSet if fv_domain == \"freq\" else RGBManipulationSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = hydra.utils.instantiate(\n",
    "    cfg.data.load_function, path=data_dir + cfg.data.data_path\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    CustomDataset(train_dataset, class_dict_file),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    CustomDataset(test_dataset, class_dict_file),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "default_model = hydra.utils.instantiate(cfg.model.model)\n",
    "if original_weights is not None:\n",
    "    default_model.load_state_dict(torch.load(original_weights, map_location=device))\n",
    "default_model.to(device)\n",
    "default_model.eval()\n",
    "\n",
    "before_acc = evaluate(default_model, test_loader, device)\n",
    "\n",
    "before_a, target_b, idxs = get_encodings(\n",
    "    default_model, cfg.model.layer, [test_loader], device\n",
    ")\n",
    "top_idxs_before = list(np.argsort(before_a[:, target_neuron])[::-1][:TOP_K])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "before_auroc = get_auroc(before_a, target_b, target_neuron)\n",
    "print(\"Before AUROC: \", before_auroc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = []\n",
    "# For each remaining parameter, iterate over its provided values.\n",
    "for j, combo in enumerate(combinations):\n",
    "    cfg, overrides = get_combo_cfg(cfg_name, cfg_path, combo)\n",
    "    PATH = path_from_cfg(cfg)\n",
    "    img_str = combo[\"img_str\"].replace(\"_gondola\", \"\")\n",
    "\n",
    "    model = hydra.utils.instantiate(cfg.model.model)\n",
    "    model.to(device)\n",
    "    model_dict = torch.load(PATH, map_location=torch.device(device))\n",
    "    model.load_state_dict(model_dict[\"model\"])\n",
    "\n",
    "    if model_dict[\"after_acc\"] is None:\n",
    "        model_dict[\"after_acc\"] = evaluate(model, test_loader, device)\n",
    "        torch.save(model_dict, PATH)\n",
    "\n",
    "    if \"auc\" not in model_dict:\n",
    "        after_a, target_a, idxs = get_encodings(model, layer_str, [test_loader], device)\n",
    "        top_idxs_after = list(np.argsort(after_a[:, target_neuron])[::-1][:TOP_K])\n",
    "        model_dict[\"auc\"] = get_auroc(after_a, target_a, target_neuron)\n",
    "        torch.save(model_dict, PATH)\n",
    "\n",
    "    cfg.target_img_path = str(img_path.with_name(img_str + \".JPEG\"))\n",
    "\n",
    "    mdict = {\n",
    "        \"model_str\": img_str,\n",
    "        \"model\": model,\n",
    "        \"acc\": model_dict[\"after_acc\"],\n",
    "        \"cfg\": cfg,\n",
    "        \"epochs\": model_dict[\"epoch\"],\n",
    "        \"auc\": model_dict[\"auc\"],\n",
    "        \"jaccard\": 0.0,\n",
    "        \"top_k_names\": [],\n",
    "    }\n",
    "    models.append(mdict)\n",
    "    print(\"Model accuracy: \", \"\\n {:0.2f} \\%\".format(model_dict[\"after_acc\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cfg.target_img_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = cfg.eval_lr\n",
    "nsteps = 500\n",
    "nvis = 5\n",
    "n_fv_obs = 5\n",
    "\n",
    "eval_fv_tuples = [  # (\"normal\", 0.001),\n",
    "    (cfg.eval_fv_dist, float(cfg.eval_fv_sd)),  # (\"normal\", 0.1), (\"normal\", 1.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Similarity Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_funcs = [\n",
    "    (r\"SSIM $\\uparrow$\", ssim_dist, r\"SSIM\"),\n",
    "    (r\"LPIPS $\\downarrow$\", alex_lpips, r\"LPIPS\"),\n",
    "    (r\"MSE $\\downarrow$\", mse_dist, r\"MSE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from experiments.eval_utils import clip_dist\n",
    "import torchvision\n",
    "\n",
    "if n_channels == 1:\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [\n",
    "            lambda x: x.repeat(1, 3, 1, 1),\n",
    "            normalize,\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [\n",
    "            normalize,\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "        ]\n",
    "    )\n",
    "clip_dist_to_target = lambda x, y: clip_dist(\n",
    "    preprocess(x),\n",
    "    preprocess(y),\n",
    ")\n",
    "\n",
    "dist_funcs.append((r\"CLIP $\\uparrow$\", clip_dist_to_target, \"CLIP\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_kwargs = define_AM_strategies(lr, nsteps, image_transforms)[STRATEGY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Qualitative Analysis: Plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df_basic = collect_fv_data(\n",
    "    models=models,\n",
    "    target_act_fn=target_act_fn,\n",
    "    fv_kwargs=fv_kwargs,\n",
    "    eval_fv_tuples=eval_fv_tuples,\n",
    "    noise_gen_class=noise_ds_type,\n",
    "    image_dims=image_dims,\n",
    "    normalize=normalize,\n",
    "    denormalize=denormalize,\n",
    "    resize_transforms=resize_transforms,\n",
    "    n_channels=n_channels,\n",
    "    layer_str=layer_str,\n",
    "    target_neuron=target_neuron,\n",
    "    n_fv_obs=30,\n",
    "    dist_funcs=dist_funcs,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df_basic_ex = results_df_basic[results_df_basic[\"iter\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from plotting import update_font\n",
    "\n",
    "\n",
    "def fv_grid_different_targets(results_df, nrows=2, ncols=7):\n",
    "    update_font(10)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(20, 4.5))\n",
    "    plt.subplots_adjust(hspace=0.0, wspace=0.25)\n",
    "\n",
    "    for idx, (i, row) in enumerate(results_df.iterrows()):\n",
    "        ax = axes[idx // ncols, idx % ncols]\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Fine-tuned y and height for compact layout\n",
    "        y_pos = 0.30\n",
    "        height = 0.66\n",
    "\n",
    "        inset_target = ax.inset_axes([0.0, y_pos, 0.49, height])\n",
    "        inset_picture = ax.inset_axes([0.51, y_pos, 0.49, height])\n",
    "\n",
    "        inset_target.imshow(row[\"target\"])\n",
    "        inset_target.axis(\"off\")\n",
    "\n",
    "        inset_picture.imshow(row[\"picture\"])\n",
    "        inset_picture.axis(\"off\")\n",
    "\n",
    "        # Bring text right under the image, super close\n",
    "        metrics_text = (\n",
    "            f\"AUC: {row['auc']:.5f}\\nACC: {row['acc']:.5f}%\\n\"\n",
    "            + row[\"LPIPS $\\downarrow$_stat\"]\n",
    "        )\n",
    "        ax.text(\n",
    "            0.76,\n",
    "            0.25,\n",
    "            metrics_text,\n",
    "            fontsize=18,\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    fig.tight_layout()  # removes outer margins\n",
    "    return fig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_table = (\n",
    "    results_df_basic.groupby([\"model\"])\n",
    "    .describe(include=[float])\n",
    "    .loc[:, (slice(None), [\"mean\", \"std\"])]\n",
    ")\n",
    "\n",
    "eval_table.columns = eval_table.columns.map(\"_\".join)\n",
    "for s in [d[0] for d in dist_funcs]:\n",
    "    eval_table[s + \"_mean\"] = eval_table[s + \"_mean\"].map(\"${:,.3f}\".format).astype(str)\n",
    "    eval_table[s + \"_std\"] = eval_table[s + \"_std\"].map(\"{:,.3f}$\".format).astype(str)\n",
    "    eval_table[s + \"_stat\"] = eval_table[s + \"_mean\"] + \"\\pm\" + eval_table[s + \"_std\"]\n",
    "\n",
    "eval_table = eval_table[[d[0] + \"_stat\" for d in dist_funcs[::-1]]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df_basic_ex = results_df_basic_ex.merge(\n",
    "    eval_table.reset_index(), on=\"model\", how=\"left\"\n",
    ")\n",
    "results_df_basic_ex[\"acc\"] = results_df_basic_ex[\"acc\"] - before_acc\n",
    "results_df_basic_ex[\"auc\"] = results_df_basic_ex[\"auc\"] - before_auroc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = fv_grid_different_targets(\n",
    "    results_df_basic_ex,\n",
    ")\n",
    "# plt.subplots_adjust(hspace=0.22, wspace=0.02)\n",
    "plt.savefig(f\"{save_path}/target_image_lpips.svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df_basic_ex[\"auc\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
