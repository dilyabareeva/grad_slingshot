{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356e22d03dbea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.manipulation_set import FrequencyManipulationSet, RGBManipulationSet\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "import torch\n",
    "import torch.multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from core.utils import read_target_image\n",
    "from experiments.eval_utils import feature_visualisation\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0296fb36951cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = compose(\n",
    "        config_name=\"config_rs50_dalmatian_tunnel\",\n",
    "        overrides=[],\n",
    "    )\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643686cc5b7f73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationUnit(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, norm_target, n_channels, image_dims, filter_dim=None, out_channels=1\n",
    "    ):\n",
    "        super(VisualizationUnit, self).__init__()\n",
    "\n",
    "        filter_dim = filter_dim or image_dims\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_dim = filter_dim\n",
    "        self.image_dims = image_dims\n",
    "        self.norm_target_shape = norm_target.shape\n",
    "\n",
    "        # Define a single Conv2d layer (no training required)\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=out_channels,  # Single output\n",
    "            kernel_size=filter_dim,\n",
    "            stride=(2, 2),\n",
    "            padding=(3, 3),\n",
    "            bias=False,\n",
    "        )\n",
    "        self.max_pool = torch.nn.MaxPool2d(\n",
    "            kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False\n",
    "        )\n",
    "        self.avg_pool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.rand_xs = [\n",
    "            0,\n",
    "            self.norm_target_shape[2] - self.filter_dim,\n",
    "            self.norm_target_shape[2] - self.filter_dim,\n",
    "            0,\n",
    "        ]\n",
    "        self.rand_ys = [\n",
    "            0,\n",
    "            self.norm_target_shape[3] - self.filter_dim,\n",
    "            0,\n",
    "            self.norm_target_shape[3] - self.filter_dim,\n",
    "        ]\n",
    "\n",
    "        # Set weights directly from norm_target\n",
    "        with torch.no_grad():\n",
    "            for n in range(self.out_channels):\n",
    "                # find a subimage of the target images in the size of the kernel\n",
    "                if n < 4:\n",
    "                    rand_x = self.rand_xs[n]\n",
    "                    rand_y = self.rand_ys[n]\n",
    "                else:\n",
    "                    rand_x = random.randint(\n",
    "                        0, self.norm_target_shape[2] - self.filter_dim\n",
    "                    )\n",
    "                    rand_y = random.randint(\n",
    "                        0, self.norm_target_shape[3] - self.filter_dim\n",
    "                    )\n",
    "                subimage = norm_target[\n",
    "                    :,\n",
    "                    :,\n",
    "                    rand_x : rand_x + self.filter_dim,\n",
    "                    rand_y : rand_y + self.filter_dim,\n",
    "                ]\n",
    "                self.conv.weight[n].copy_(subimage[0])\n",
    "\n",
    "            self.max = self.avg_pool(self.max_pool(self.conv(norm_target)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolution\n",
    "        return -self.relu(-self.avg_pool(self.max_pool(self.conv(x))) + self.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4b8c28063773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cfg.device\n",
    "dataset = cfg.data\n",
    "image_dims = int(cfg.data.image_dims)\n",
    "n_channels = int(cfg.data.n_channels)\n",
    "fv_sd = float(cfg.fv_sd)\n",
    "fv_dist = cfg.fv_dist\n",
    "fv_domain = cfg.fv_domain\n",
    "target_img_path = \".\" + cfg.target_img_path\n",
    "zero_rate = cfg.get(\"zero_rate\", 0.5)\n",
    "tunnel = cfg.get(\"tunnel\", False)\n",
    "target_noise = float(cfg.get(\"target_noise\", 0.0))\n",
    "\n",
    "transforms = hydra.utils.instantiate(dataset.fv_transforms)\n",
    "normalize = hydra.utils.instantiate(cfg.data.normalize)\n",
    "denormalize = hydra.utils.instantiate(cfg.data.denormalize)\n",
    "resize_transforms = hydra.utils.instantiate(cfg.data.resize_transforms)\n",
    "\n",
    "if \"target_act_fn\" in cfg.model:\n",
    "    if \"probe_path\" in cfg.model.target_act_fn:\n",
    "        cfg.model.target_act_fn.probe_path = \".\" + cfg.model.target_act_fn.probe_path\n",
    "    target_act_fn = hydra.utils.instantiate(cfg.model.target_act_fn)\n",
    "else:\n",
    "    target_act_fn = lambda x: x\n",
    "\n",
    "default_model = hydra.utils.instantiate(cfg.model.model)\n",
    "default_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04171dfa0098df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ds_type = FrequencyManipulationSet if fv_domain == \"freq\" else RGBManipulationSet\n",
    "noise_dataset = noise_ds_type(\n",
    "    image_dims,\n",
    "    target_img_path,\n",
    "    normalize,\n",
    "    denormalize,\n",
    "    transforms,\n",
    "    resize_transforms,\n",
    "    n_channels,\n",
    "    fv_sd,\n",
    "    fv_dist,\n",
    "    zero_rate,\n",
    "    tunnel,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91148c3263c18796",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_target, _ = read_target_image(device, n_channels, target_img_path, normalize)\n",
    "# create conv layer encoding norm_target\n",
    "model = VisualizationUnit(norm_target.to(\"cpu\"), n_channels, image_dims, 64, 512)\n",
    "\n",
    "model.to(device)\n",
    "model.conv.to(device)\n",
    "model.avg_pool = model.avg_pool.to(device)\n",
    "model.max_pool = model.max_pool.to(device)\n",
    "model.max = model.max.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbc63806eb7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _, tstart = feature_visualisation(\n",
    "    net=model,\n",
    "    noise_dataset=noise_dataset,\n",
    "    man_index=0,\n",
    "    lr=0.001,\n",
    "    n_steps=100,\n",
    "    target_act_fn=target_act_fn,\n",
    "    grad_clip=True,\n",
    "    adam=True,\n",
    "    device=device,\n",
    ")\n",
    "plt.imshow(img[0].permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7011806b2715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 164\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=n_channels,\n",
    "    out_channels=2,  # Single output\n",
    "    kernel_size=kernel_size,\n",
    "    # Kernel size matches the input spatial dimensions\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    bias=False,  # No bias for simplicity\n",
    ")\n",
    "for n in range(2):\n",
    "    # find a subimage of the target images in the size of the kernel\n",
    "    rand_x = random.randint(0, norm_target.shape[2] - kernel_size)\n",
    "    rand_y = random.randint(0, norm_target.shape[3] - kernel_size)\n",
    "    subimage = norm_target[\n",
    "        :, :, rand_x : rand_x + kernel_size, rand_y : rand_y + kernel_size\n",
    "    ]\n",
    "    conv.weight.data[n] = subimage\n",
    "\n",
    "conv(norm_target.to(\"cpu\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3806abbadd0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3062d81eae9fa80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
